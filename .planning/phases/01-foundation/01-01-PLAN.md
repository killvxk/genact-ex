---
phase: 01-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - data/llm_models.txt
  - data/gpu_models.txt
  - data/llm_datasets.txt
  - src/data.rs
  - src/modules/llm_training.rs
  - src/modules/mod.rs
autonomous: true

must_haves:
  truths:
    - "Module appears in `genact --list` output"
    - "Running `cargo run -- -m llm_training` produces output"
    - "Module compiles for native target"
    - "Module responds to CTRL-C (should_exit check)"
  artifacts:
    - path: "data/llm_models.txt"
      provides: "LLM model names (real + funny)"
      min_lines: 30
    - path: "data/gpu_models.txt"
      provides: "NVIDIA GPU model names"
      min_lines: 15
    - path: "data/llm_datasets.txt"
      provides: "Dataset names (real + fictional)"
      min_lines: 25
    - path: "src/modules/llm_training.rs"
      provides: "LLM training module implementation"
      exports: ["LlmTraining"]
    - path: "src/data.rs"
      provides: "Data file loading"
      contains: "LLM_MODELS_LIST"
    - path: "src/modules/mod.rs"
      provides: "Module registration"
      contains: "llm_training"
  key_links:
    - from: "src/modules/llm_training.rs"
      to: "src/data.rs"
      via: "use crate::data::{LLM_MODELS_LIST, GPU_MODELS_LIST, LLM_DATASETS_LIST}"
      pattern: "use crate::data::\\{.*LLM_MODELS_LIST"
    - from: "src/modules/mod.rs"
      to: "src/modules/llm_training.rs"
      via: "pub mod declaration and ALL_MODULES insert"
      pattern: "pub mod llm_training"
---

<objective>
Create the foundational data files and module skeleton for the LLM training simulator.

Purpose: Establish the module structure so future phases can build upon it. This is the minimal working module that proves the pattern works.

Output: A working (but minimal) llm_training module that appears in `genact --list` and produces placeholder output when run.
</objective>

<execution_context>
@C:\Users\root\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\root\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/01-RESEARCH.md

# Existing patterns to follow
@src/data.rs
@src/modules/mod.rs
@src/modules/bootlog.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create data files</name>
  <files>
    data/llm_models.txt
    data/gpu_models.txt
    data/llm_datasets.txt
  </files>
  <action>
Create three data files in the `data/` directory following the existing pattern (one item per line, no trailing newlines).

**llm_models.txt** (~35 lines): Mix of real and funny model names
- Real models: GPT-4, GPT-4o, LLaMA-70B, LLaMA-3-405B, Claude-3, Mistral-7B, Mixtral-8x7B, BERT-Large, Falcon-180B, MPT-30B, Gemma-7B, Qwen-72B, Yi-34B, DeepSeek-67B, Phi-3, Command-R+
- Funny models: OverfittedTransformer-9000, GigaChadLM-Ultimate, HallucinationMaster-XL, TurboSlothAI-7B, QuantumVibeNet-Pro, ChonkyBOT-Prime, MegaBrainGPT-Omega, UltraThinkinator-3000, SentientToaster-7B, CopyCat-Turbo, ConfusedPanda-13B, LazyNeuron-XL, AggressiveAutocomplete-9B, ThinkingTooHard-405B, MemoryLeakLM-128B

**gpu_models.txt** (~18 lines): NVIDIA datacenter GPUs with memory variants
- V100 16GB, V100 32GB, T4, A10, A30, A40, A100 40GB, A100 80GB, H100 80GB, H100 NVL, H200, L4, L40, L40S, B100, B200, GB200, DGX H100

**llm_datasets.txt** (~30 lines): Mix of real and fictional datasets
- Real datasets: CommonCrawl, C4, The Pile, Wikipedia-en, BookCorpus, OpenWebText, RedPajama, LAION-5B, RefinedWeb, SlimPajama, StarCoder-Data, ROOTS, mC4, CC-News, PubMed-Abstracts
- Fictional datasets: UltraCorpus-2025, MegaText-Infinity, InternetDump-Pro, AllTheBooks-v3, RandomThoughts-XL, HumanWisdom-Dataset, TotalKnowledge-9000, StackOverflow-Complete, TwitterFirehose-Unfiltered, RedditThoughts-Archive, GitHubCode-Everything, ArXiv-AllTime, YouTube-Transcripts-Mega

Ensure no blank lines and no trailing newline at end of file.
  </action>
  <verify>
```bash
# Check files exist and have content
wc -l data/llm_models.txt data/gpu_models.txt data/llm_datasets.txt
# Verify no empty lines
grep -c "^$" data/llm_models.txt data/gpu_models.txt data/llm_datasets.txt || echo "No empty lines (good)"
```
  </verify>
  <done>
Three data files exist with 30+, 15+, and 25+ lines respectively. No empty lines.
  </done>
</task>

<task type="auto">
  <name>Task 2: Register data files in data.rs</name>
  <files>src/data.rs</files>
  <action>
Edit `src/data.rs` to add the three new data files.

1. Add static includes near the top (after line 22, before the lazy_static block):
```rust
static LLM_MODELS: &str = include_str!("../data/llm_models.txt");
static GPU_MODELS: &str = include_str!("../data/gpu_models.txt");
static LLM_DATASETS: &str = include_str!("../data/llm_datasets.txt");
```

2. Add to the lazy_static! block (before the closing brace, around line 55):
```rust
    pub static ref LLM_MODELS_LIST: Vec<&'static str> = LLM_MODELS.lines().collect();
    pub static ref GPU_MODELS_LIST: Vec<&'static str> = GPU_MODELS.lines().collect();
    pub static ref LLM_DATASETS_LIST: Vec<&'static str> = LLM_DATASETS.lines().collect();
```

Follow the existing pattern exactly - static include at top, lazy_static vector in the block.
  </action>
  <verify>
```bash
# Check compilation succeeds
cargo check 2>&1 | head -20
# Verify the exports exist
grep -n "LLM_MODELS_LIST\|GPU_MODELS_LIST\|LLM_DATASETS_LIST" src/data.rs
```
  </verify>
  <done>
`cargo check` succeeds. All three _LIST exports appear in data.rs.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create llm_training module and register it</name>
  <files>
    src/modules/llm_training.rs
    src/modules/mod.rs
  </files>
  <action>
1. Create `src/modules/llm_training.rs` following the bootlog.rs pattern:

```rust
//! Pretend to train a large language model
use async_trait::async_trait;
use rand::seq::IndexedRandom;
use rand::rng;

use crate::args::AppConfig;
use crate::data::{GPU_MODELS_LIST, LLM_DATASETS_LIST, LLM_MODELS_LIST};
use crate::io::{csleep, newline, print};
use crate::modules::Module;

pub struct LlmTraining;

#[async_trait(?Send)]
impl Module for LlmTraining {
    fn name(&self) -> &'static str {
        "llm_training"
    }

    fn signature(&self) -> String {
        "python train.py --model llama-7b --gpus 64".to_string()
    }

    async fn run(&self, appconfig: &AppConfig) {
        let mut rng = rng();

        // Select random data
        let model = LLM_MODELS_LIST.choose(&mut rng).unwrap_or(&"GPT-Unknown");
        let gpu = GPU_MODELS_LIST.choose(&mut rng).unwrap_or(&"A100");
        let dataset = LLM_DATASETS_LIST.choose(&mut rng).unwrap_or(&"CommonCrawl");

        // Placeholder output - will be expanded in Phase 2
        print(format!("[INFO] Loading model: {model}")).await;
        newline().await;
        csleep(500).await;

        print(format!("[INFO] Detected 64x NVIDIA {gpu}")).await;
        newline().await;
        csleep(300).await;

        print(format!("[INFO] Dataset: {dataset}")).await;
        newline().await;
        csleep(300).await;

        // Simple loop with exit check
        for epoch in 1..=3 {
            print(format!("[TRAIN] Epoch {epoch}/3 - Training...")).await;
            newline().await;
            csleep(1000).await;

            if appconfig.should_exit() {
                return;
            }
        }

        print("[INFO] Training placeholder complete").await;
        newline().await;
    }
}
```

2. Edit `src/modules/mod.rs`:
   - Add `pub mod llm_training;` in alphabetical order (after `pub mod kernel_compile;`, before `pub mod memdump;`)
   - Add to ALL_MODULES: `all_modules.insert("llm_training", Box::new(llm_training::LlmTraining));` (after kernel_compile, before memdump)

Keep alphabetical order to match existing convention.
  </action>
  <verify>
```bash
# Full compilation check
cargo build 2>&1 | tail -10

# Verify module appears in list
cargo run -- --list-modules 2>&1 | grep llm_training

# Run the module briefly
timeout 5 cargo run -- -m llm_training 2>&1 || echo "Timeout reached (expected)"
```
  </verify>
  <done>
`cargo build` succeeds. `llm_training` appears in module list. Running the module produces output.
  </done>
</task>

</tasks>

<verification>
After all tasks complete, verify phase success criteria:

```bash
# 1. Module appears in genact --list
cargo run -- --list-modules 2>&1 | grep -q llm_training && echo "PASS: Module in list" || echo "FAIL: Module not in list"

# 2. Module produces output
timeout 3 cargo run -- -m llm_training 2>&1 | grep -q "Loading model" && echo "PASS: Module produces output" || echo "FAIL: No output"

# 3. Data files loaded (compilation would fail if not)
echo "PASS: Data files loaded (compilation succeeded)"

# 4. Clippy check (warnings allowed in Phase 1, strict in Phase 4)
cargo clippy 2>&1 | tail -5

# 5. Formatting check
cargo fmt --all -- --check && echo "PASS: Formatting OK" || echo "WARN: Formatting issues"
```
</verification>

<success_criteria>
- `cargo build` succeeds with no errors
- `cargo run -- --list-modules` shows `llm_training`
- `cargo run -- -m llm_training` displays model/GPU/dataset info and epoch progress
- No compilation errors related to data file loading
- Module responds to CTRL-C during execution (should_exit check in loop)
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-01-SUMMARY.md`
</output>
