---
phase: 02-training-loop
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/modules/llm_training.rs
autonomous: true

must_haves:
  truths:
    - "User sees model loading with name, parameter count, and architecture"
    - "User sees tokenizer loading progress"
    - "User sees dataset loading info with sample count"
    - "User sees distributed environment initialization (NCCL, GPU count, nodes)"
    - "User sees GPU detection for 64+ GPUs"
  artifacts:
    - path: "src/modules/llm_training.rs"
      provides: "Initialization phase functions"
      contains: "run_initialization"
  key_links:
    - from: "LlmTraining::run"
      to: "run_initialization"
      via: "async function call"
      pattern: "run_initialization.*await"
---

<objective>
Implement the initialization phase of LLM training simulation

Purpose: Users see realistic PyTorch DDP style startup logs including model loading with progress bar, tokenizer/dataset info, distributed NCCL initialization, and 64+ GPU detection - making the training startup look authentic.

Output: Expanded llm_training.rs with complete initialization sequence before training begins
</objective>

<execution_context>
@C:\Users\root\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\root\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-training-loop/02-CONTEXT.md
@.planning/phases/02-training-loop/02-RESEARCH.md
@.planning/phases/01-foundation/01-01-SUMMARY.md
@src/modules/llm_training.rs
@src/modules/julia.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add initialization helper functions and imports</name>
  <files>src/modules/llm_training.rs</files>
  <action>
Add necessary imports at top of file:
- `use chrono::Local;` for timestamps
- `use yansi::Paint;` for colored output
- `use progress_string::BarBuilder;` for progress bars
- `use crate::io::{cursor_up, erase_line};` (add to existing io import)

Create helper function for timestamp + rank logging:
```rust
async fn log_info(message: &str) {
    let ts = Local::now().format("%Y-%m-%d %H:%M:%S");
    print(format!("[{}] {} {}", ts, Paint::green("INFO").bold(), message)).await;
    newline().await;
}

async fn log_info_rank(rank: u32, world_size: u32, message: &str) {
    let ts = Local::now().format("%Y-%m-%d %H:%M:%S");
    print(format!("[{}] [Rank {}/{}] {} {}", ts, rank, world_size, Paint::green("INFO").bold(), message)).await;
    newline().await;
}

async fn log_warning(message: &str) {
    let ts = Local::now().format("%Y-%m-%d %H:%M:%S");
    print(format!("[{}] {} {}", ts, Paint::yellow("WARNING").bold(), message)).await;
    newline().await;
}
```

These helper functions follow the pattern established in julia.rs (log_action) and provide the PyTorch DDP style output format specified in CONTEXT.md.
  </action>
  <verify>File compiles: `cargo check`</verify>
  <done>Helper functions exist and module compiles</done>
</task>

<task type="auto">
  <name>Task 2: Implement run_initialization function</name>
  <files>src/modules/llm_training.rs</files>
  <action>
Create `run_initialization` async function that displays the complete initialization sequence. Follow CONTEXT.md decisions and RESEARCH.md patterns:

1. **Model Loading** (INIT-01):
   - Select random model from LLM_MODELS_LIST
   - Generate random parameter count (7B-405B)
   - Generate architecture details: layers (32-128), hidden_size (4096-16384), attention_heads (32-128)
   - Display model loading with progress bar using BarBuilder (40 chars wide, include_percent)
   - Show "Model loaded in X.Xs" completion message

2. **Tokenizer Loading** (INIT-02):
   - Display tokenizer name (derived from model name or "SentencePiece")
   - Show vocab size (32000-128000)

3. **Dataset Loading** (INIT-03):
   - Select random dataset from LLM_DATASETS_LIST
   - Generate sample count (1M-100M)
   - Show batch configuration (batch_size=2048, micro_batch=4)

4. **Distributed Environment** (INIT-04):
   - Display NCCL version (2.19.3)
   - Show world_size (64 GPUs), num_nodes (8)
   - Run NCCL AllReduce test with timing

5. **GPU Detection** (INIT-05):
   - Show first 16 GPUs with rank/node info
   - Display "... and N more GPUs" for remaining
   - Use GPU_MODELS_LIST for GPU type

Include should_exit() checks after each major section. Use csleep() for realistic timing between outputs.

Progress bar update pattern (from julia.rs):
```rust
for i in 0..=100 {
    bar.replace(i);
    erase_line().await;
    print(format!("[{}] {} Loading model weights... {}", ts, Paint::green("INFO").bold(), bar)).await;
    csleep(rng.random_range(20..80)).await;
    if appconfig.should_exit() { newline().await; return; }
}
newline().await;
```
  </action>
  <verify>`cargo run -- -m llm_training` shows initialization sequence with model loading progress bar, distributed init, and GPU detection</verify>
  <done>Running module shows model loading (name, params, architecture), tokenizer, dataset, NCCL init, and 64+ GPU detection</done>
</task>

<task type="auto">
  <name>Task 3: Update Module::run to call initialization</name>
  <files>src/modules/llm_training.rs</files>
  <action>
Replace the current placeholder implementation in `impl Module for LlmTraining`:

1. Move the current run() body to run_initialization()
2. Update run() to call run_initialization(appconfig).await
3. Add a placeholder comment for training loop (Phase 2 Plan 02)

Structure:
```rust
async fn run(&self, appconfig: &AppConfig) {
    // Phase 2.1: Initialization
    run_initialization(appconfig).await;

    if appconfig.should_exit() {
        return;
    }

    // Phase 2.2: Training Loop (02-02-PLAN.md)
    // Placeholder - training loop will be added in next plan
    print("[INFO] Training placeholder - see 02-02-PLAN.md").await;
    newline().await;
}
```

Run `cargo fmt --all` and `cargo clippy -- -D warnings` to ensure code quality.
  </action>
  <verify>`cargo run -- -m llm_training` shows full initialization then placeholder message. `cargo clippy -- -D warnings` passes.</verify>
  <done>Module shows initialization sequence, passes clippy, and has clear placeholder for training loop</done>
</task>

</tasks>

<verification>
Run complete verification:
```bash
cargo fmt --all -- --check
cargo clippy -- -D warnings
cargo run -- -m llm_training
```

Expected output should include:
1. Timestamp-prefixed log lines with [INFO] tags
2. Model loading with name, parameter count, architecture details
3. Progress bar for model weight loading
4. Tokenizer name and vocab size
5. Dataset name and sample count
6. NCCL version and world size
7. GPU detection for multiple ranks
8. "... and N more GPUs" message
9. AllReduce test result
</verification>

<success_criteria>
- All 5 INIT requirements (INIT-01 through INIT-05) are visible in output
- PyTorch DDP style timestamps and formatting match CONTEXT.md decisions
- Progress bar updates smoothly without flickering
- should_exit() is checked throughout initialization
- Code passes cargo fmt and clippy checks
</success_criteria>

<output>
After completion, create `.planning/phases/02-training-loop/02-01-SUMMARY.md`
</output>
