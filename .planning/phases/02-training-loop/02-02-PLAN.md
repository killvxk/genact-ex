---
phase: 02-training-loop
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/modules/llm_training.rs
autonomous: true

must_haves:
  truths:
    - "User sees epoch progress bar advancing with percentage"
    - "User sees step progress with loss, perplexity, learning rate, tokens/s"
    - "User sees training loss decreasing realistically over steps with fluctuation"
    - "User sees GPU status grid showing memory, utilization, temperature for 64+ GPUs"
    - "User sees time elapsed and ETA estimates"
    - "User sees occasional NCCL AllReduce communication logs"
  artifacts:
    - path: "src/modules/llm_training.rs"
      provides: "Training loop with metrics and GPU status"
      contains: "run_training_loop"
  key_links:
    - from: "LlmTraining::run"
      to: "run_training_loop"
      via: "async function call after initialization"
      pattern: "run_training_loop.*await"
    - from: "run_training_loop"
      to: "display_gpu_status_grid"
      via: "periodic call every 10 steps"
      pattern: "display_gpu_status_grid.*await"
---

<objective>
Implement the training loop with metrics display and GPU status grid

Purpose: Users see convincing training progress with dual progress bars (epoch/step), fluctuating loss metrics, speed statistics, and a 64+ GPU status grid grouped by node - the core visual impact of LLM training simulation.

Output: Complete training loop implementation in llm_training.rs with realistic metrics and GPU monitoring
</objective>

<execution_context>
@C:\Users\root\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\root\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-training-loop/02-CONTEXT.md
@.planning/phases/02-training-loop/02-RESEARCH.md
@.planning/phases/02-training-loop/02-01-SUMMARY.md
@src/modules/llm_training.rs
@src/modules/julia.rs
@src/modules/cryptomining.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add GpuStatus struct and loss generation helpers</name>
  <files>src/modules/llm_training.rs</files>
  <action>
Add imports needed for training loop:
- `use rand_distr::{Distribution, Normal};` for loss noise and GPU stat variation
- `use instant::Instant;` for elapsed time tracking
- `use rand::Rng;` for random_range (if not already imported)

Create GpuStatus struct for GPU monitoring:
```rust
struct GpuStatus {
    id: u32,
    node: u32,
    memory_used_gb: f32,
    memory_total_gb: f32,
    utilization: f32,
    temperature: u32,
}
```

Create function to generate GPU statuses with variation (from RESEARCH.md pattern):
```rust
fn generate_gpu_statuses(num_gpus: u32, rng: &mut impl Rng) -> Vec<GpuStatus> {
    let base_util = rng.random_range(90.0..98.0);
    let base_temp = rng.random_range(70..80);
    let base_mem = rng.random_range(70.0..78.0);
    let total_mem = 80.0f32;

    let util_noise = Normal::new(0.0, 2.0).unwrap();
    let temp_noise = Normal::new(0.0, 3.0).unwrap();
    let mem_noise = Normal::new(0.0, 1.5).unwrap();

    (0..num_gpus).map(|id| {
        GpuStatus {
            id,
            node: id / 8,
            memory_used_gb: (base_mem + mem_noise.sample(rng) as f32).clamp(60.0, total_mem - 1.0),
            memory_total_gb: total_mem,
            utilization: (base_util + util_noise.sample(rng)).clamp(0.0, 100.0) as f32,
            temperature: (base_temp as f64 + temp_noise.sample(rng)).clamp(50.0, 95.0) as u32,
        }
    }).collect()
}
```

This ensures each GPU has slightly different values (avoiding the "all identical" pitfall from RESEARCH.md).
  </action>
  <verify>File compiles: `cargo check`</verify>
  <done>GpuStatus struct and generate_gpu_statuses function exist and compile</done>
</task>

<task type="auto">
  <name>Task 2: Implement GPU status grid display function</name>
  <files>src/modules/llm_training.rs</files>
  <action>
Create `display_gpu_status_grid` async function following CONTEXT.md and RESEARCH.md patterns:

```rust
async fn display_gpu_status_grid(gpus: &[GpuStatus], num_nodes: u32) {
    print(format!("{}", Paint::cyan("[ GPU Status Grid ]").bold())).await;
    newline().await;

    for node in 0..num_nodes {
        let node_gpus: Vec<_> = gpus.iter().filter(|g| g.node == node).collect();

        print(format!("Node {}: ", node)).await;

        for gpu in node_gpus {
            let mem_pct = (gpu.memory_used_gb / gpu.memory_total_gb * 100.0) as u32;

            // Color based on health (CONTEXT.md decision)
            let status_color = if gpu.temperature > 85 || mem_pct > 95 {
                yansi::Color::Red
            } else if gpu.temperature > 75 || mem_pct > 85 {
                yansi::Color::Yellow
            } else {
                yansi::Color::Green
            };

            let status_char = Paint::new("*").fg(status_color);

            print(format!(
                "{} GPU{}: {}% {}C {:.0}/{:.0}GB  ",
                status_char,
                gpu.id % 8,
                gpu.utilization as u32,
                gpu.temperature,
                gpu.memory_used_gb,
                gpu.memory_total_gb
            )).await;
        }
        newline().await;
    }
}
```

Key features:
- Groups by node (8 GPUs per node)
- Color-coded health status (green/yellow/red)
- Shows utilization %, temperature, memory usage
- Follows CONTEXT.md decision for node-grouped display
  </action>
  <verify>File compiles: `cargo check`</verify>
  <done>display_gpu_status_grid function exists and compiles</done>
</task>

<task type="auto">
  <name>Task 3: Implement run_training_loop function</name>
  <files>src/modules/llm_training.rs</files>
  <action>
Create `run_training_loop` async function with dual progress bars and metrics. Follow CONTEXT.md decisions and RESEARCH.md patterns closely:

**Configuration:**
- total_epochs: 3
- steps_per_epoch: random 500-2000 (CONTEXT.md: medium count)
- num_gpus: 64, num_nodes: 8

**Loss Generation (TRAIN-07):**
- Initial loss: 8.0-12.0
- Exponential decay with noise using Normal distribution
- Pattern: `loss = base * exp(-decay_rate * step) * (1.0 + noise)`
- Floor at 0.1 to prevent negative

**Progress Bars (TRAIN-01):**
- Epoch bar: 25 chars, include_numbers
- Step bar: 35 chars, include_percent
- Update using cursor_up(2) + erase_line pattern from julia.rs

**Metrics Display (TRAIN-02 through TRAIN-06):**
- Loss (4 decimal places)
- Perplexity: exp(loss)
- Learning rate: 1e-4 (scientific notation)
- Tokens/sec: 800K-1.2M
- Elapsed time and ETA

**Periodic Output (CONTEXT.md: every 10 steps):**
- Detailed log line with step/loss/ppl/grad_norm
- GPU status grid display
- Occasional NCCL AllReduce log (random, ~every 50-100 steps)
- Occasional WARNING log for realism (~5% chance)

**Loop Structure:**
```rust
for epoch in 1..=total_epochs {
    for step in 1..=steps_per_epoch {
        // Update loss with decay + noise
        // Update progress bars with cursor manipulation
        // Print metrics inline

        if step % 10 == 0 {
            // Detailed log
            // GPU status grid (regenerate with small variation)
        }

        // Occasional NCCL log
        if rng.random_bool(0.02) {
            log_info(&format!("NCCL AllReduce completed in {:.2}ms", rng.random_range(2.0..15.0))).await;
        }

        // Occasional warning
        if rng.random_bool(0.005) {
            log_warning("GPU memory usage approaching limit").await;
        }

        if appconfig.should_exit() {
            newline().await;
            return;
        }

        csleep(rng.random_range(30..80)).await;
    }

    // Epoch summary
}
```

Ensure cursor_up and erase_line calls match the julia.rs pattern to avoid flickering.
  </action>
  <verify>`cargo check` passes; function structure is complete</verify>
  <done>run_training_loop function implements all TRAIN and GPU requirements</done>
</task>

<task type="auto">
  <name>Task 4: Wire training loop into Module::run and verify</name>
  <files>src/modules/llm_training.rs</files>
  <action>
Update the Module::run implementation to call run_training_loop after initialization:

```rust
async fn run(&self, appconfig: &AppConfig) {
    // Phase 2.1: Initialization
    run_initialization(appconfig).await;

    if appconfig.should_exit() {
        return;
    }

    // Phase 2.2: Training Loop
    run_training_loop(appconfig).await;
}
```

Run full verification:
1. `cargo fmt --all` - format code
2. `cargo clippy -- -D warnings` - lint check
3. `cargo run -- -m llm_training` - visual verification

Watch for:
- Smooth progress bar updates without flickering
- Loss decreasing with visible fluctuation
- GPU grid displaying 8 nodes with 8 GPUs each
- Colored status indicators (green/yellow)
- Timestamps on all log lines
  </action>
  <verify>
```bash
cargo fmt --all -- --check
cargo clippy -- -D warnings
cargo run -- -m llm_training
```
All pass; output shows initialization followed by training loop with progress bars, metrics, and GPU grid.
  </verify>
  <done>
Complete training simulation visible:
- Dual progress bars (epoch + step)
- Loss decreasing with fluctuation
- Perplexity, LR, tokens/s metrics
- Elapsed/ETA time estimates
- GPU status grid for 64 GPUs
- NCCL logs and occasional warnings
  </done>
</task>

</tasks>

<verification>
Run complete verification:
```bash
cargo fmt --all -- --check
cargo clippy -- -D warnings
cargo run -- -m llm_training
```

Expected output flow:
1. Initialization sequence (from 02-01)
2. "======== Training Started ========"
3. Epoch progress bar: `Epoch: [====...] 1/3`
4. Step progress bar with metrics: `Step: [====...] 50% | Loss: 8.2345 | PPL: 3456.78 | LR: 1.00e-04 | 950000tok/s | 45s<120s`
5. Every 10 steps: detailed log line + GPU status grid
6. Occasional NCCL AllReduce messages
7. Epoch summary at end of each epoch
8. Continues until all epochs complete or CTRL-C
</verification>

<success_criteria>
- All TRAIN requirements (TRAIN-01 through TRAIN-07) visible in output
- All GPU requirements (GPU-01 through GPU-04) visible in GPU grid
- Loss decreases with realistic fluctuation (not perfectly smooth)
- GPU stats vary between GPUs (not all identical)
- Progress bars update smoothly without artifacts
- should_exit() checked in inner loop for responsive CTRL-C
- Passes cargo fmt and clippy checks
</success_criteria>

<output>
After completion, create `.planning/phases/02-training-loop/02-02-SUMMARY.md`
</output>
