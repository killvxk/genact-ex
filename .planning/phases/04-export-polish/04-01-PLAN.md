---
phase: 04-export-polish
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/modules/llm_training.rs
autonomous: true

must_haves:
  truths:
    - "User sees multi-step export process (merge weights, optimize, serialize, write)"
    - "User sees shard file exports with progress bars (model-00001-of-00004.safetensors)"
    - "User sees companion files saved (config.json, tokenizer.json, etc.)"
    - "User sees training completion summary with statistics table"
    - "Training restarts in infinite loop after completion"
    - "cargo clippy -- -D warnings passes with no errors"
  artifacts:
    - path: "src/modules/llm_training.rs"
      provides: "Export phase and infinite loop logic"
      contains: "run_export_phase"
  key_links:
    - from: "run()"
      to: "run_export_phase()"
      via: "Called after run_training_loop completes"
    - from: "run()"
      to: "loop"
      via: "Outer infinite loop wraps all phases"
---

<objective>
Implement the export phase after training completion and add infinite loop behavior.

Purpose: Complete the LLM training simulation by showing model export (format conversion, shard writing) and training summary, then looping to start fresh training runs.

Output: Updated llm_training.rs with run_export_phase() function, training summary display, and infinite loop in run().
</objective>

<execution_context>
@C:\Users\root\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\root\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-export-polish/04-CONTEXT.md
@src/modules/llm_training.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement run_export_phase function</name>
  <files>src/modules/llm_training.rs</files>
  <action>
Create async fn run_export_phase(appconfig: &AppConfig, model_name: &str, params_b: f64, train_loss: f64, train_time_secs: f64, total_epochs: u32, total_steps: u32).

Implementation per CONTEXT.md:

1. Visual separator: "=============== Export Phase ===============" in cyan

2. Multi-step export with progress bars for each:
   a. "Merging distributed weights..." - slowest step (~3-5 sec)
   b. "Optimizing model for inference..." - medium (~2-3 sec)
   c. "Serializing to SafeTensors format..." - medium (~2-3 sec)
   d. "Writing model shards..." - fastest (~1-2 sec)

   Each step uses BarBuilder with width=30, shows progress 0-100%.

3. Shard file export display:
   - Calculate num_shards from params_b (roughly params_b * 2 / 5 GB per shard, min 1, max 8)
   - For each shard: log_info "Saved: model-{N:05}-of-{total:05}.safetensors ({size:.1}GB)"
   - Size per shard = (params_b * 2.0) / num_shards GB

4. Companion files display (use log_info with checkmark style):
   - "Saved: config.json"
   - "Saved: tokenizer.json"
   - "Saved: generation_config.json"
   - "Saved: model.safetensors.index.json"

5. Export complete message with total size

6. Visual separator before summary

7. Training summary table (PyTorch Lightning style):
   ```
   ┌─────────────────────────────────────────┐
   │         Training Summary                 │
   ├─────────────────────────────────────────┤
   │ Total time:     {formatted_time}         │
   │ Epochs:         {epochs}                 │
   │ Steps:          {steps}                  │
   │ Final loss:     {loss:.4}                │
   │ Final PPL:      {ppl:.2}                 │
   │ Avg GPU util:   {util}%                  │
   │ Peak memory:    {mem:.1}GB               │
   │ Model saved:    ./outputs/{name}/final/  │
   │ Total size:     {size:.1}GB              │
   └─────────────────────────────────────────┘
   ```
   Use simple ASCII borders. Format time as "Xh Ym Zs" or "Ym Zs" if < 1 hour.

8. Success message: Paint::green("✔ Training completed successfully!").bold()

Check appconfig.should_exit() at key points. Use io::* functions for WASM compatibility.
  </action>
  <verify>
Function compiles with `cargo check`. Visual inspection of added code shows all elements from CONTEXT.md.
  </verify>
  <done>
run_export_phase function exists with: multi-step progress bars, shard exports, companion files, summary table, success message.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update run() to add infinite loop and call export phase</name>
  <files>src/modules/llm_training.rs</files>
  <action>
Modify the run() method in impl Module for LlmTraining:

1. Store training context variables at the top of run():
   - Generate model_name, params_b once per loop iteration (for export phase)
   - Track train_time and other stats needed for export

2. Wrap the entire run body in `loop { ... }`:

```rust
async fn run(&self, appconfig: &AppConfig) {
    loop {
        // Store context for export
        let mut rng = rng();
        let model_name = LLM_MODELS_LIST.choose(&mut rng).unwrap_or(&"GPT-Unknown");
        let params_b: f64 = rng.random_range(7.0..405.0);

        let train_start = Instant::now();

        // Phase: Initialization
        run_initialization(appconfig).await;

        if appconfig.should_exit() {
            return;
        }

        // Phase: Training Loop (returns final loss and total steps)
        let (final_loss, total_epochs, total_steps) = run_training_loop(appconfig).await;

        if appconfig.should_exit() {
            return;
        }

        let train_time = train_start.elapsed().as_secs_f64();

        // Phase: Export
        run_export_phase(appconfig, model_name, params_b, final_loss, train_time, total_epochs, total_steps).await;

        if appconfig.should_exit() {
            return;
        }

        // Transition to next training run
        newline().await;
        log_info("Starting new training run...").await;
        csleep(2000).await;
        newline().await;
    }
}
```

3. Update run_training_loop to return (f64, u32, u32) tuple:
   - Return (final_loss, total_epochs, total_steps) at the end
   - Change function signature to: `async fn run_training_loop(appconfig: &AppConfig) -> (f64, u32, u32)`

4. Update run_initialization to accept model_name and params_b as parameters (optional - can also regenerate inside for visual variety, Claude's discretion).

Note: The existing "======== Training Complete ========" at end of run_training_loop should remain as-is - export phase follows it.
  </action>
  <verify>
`cargo check` passes. Running `cargo run -- -m llm_training` for ~30 seconds shows training completing and export phase starting.
  </verify>
  <done>
run() contains infinite loop. Export phase called after training loop. Training loop returns stats. Loop transition message displayed.
  </done>
</task>

<task type="auto">
  <name>Task 3: Run clippy and fix any warnings</name>
  <files>src/modules/llm_training.rs</files>
  <action>
Run `cargo clippy -- -D warnings` and fix all warnings.

Common clippy issues to watch for in this codebase:
- Unused variables (prefix with _)
- Unnecessary clone() calls
- Use of format! where &str would work
- Redundant pattern matching
- Single-character variable names in complex contexts

After fixing, re-run clippy to confirm zero warnings. Also run `cargo fmt --all -- --check` to ensure formatting is correct.

If any warnings remain, iterate until both pass cleanly.
  </action>
  <verify>
`cargo clippy -- -D warnings` exits with code 0 and produces no warning output.
`cargo fmt --all -- --check` exits with code 0.
  </verify>
  <done>
Both clippy and fmt checks pass with no warnings or errors.
  </done>
</task>

</tasks>

<verification>
1. `cargo check` passes
2. `cargo clippy -- -D warnings` passes with no warnings
3. `cargo fmt --all -- --check` passes
4. `cargo run -- -m llm_training` shows:
   - Initialization phase
   - Training loop with metrics
   - Validation after epochs
   - Export phase with multi-step progress
   - Training summary table
   - Success message
   - Loop restart message
   - New training run begins
</verification>

<success_criteria>
- EXPORT-01: User sees model export process (merge, optimize, serialize, write shards)
- EXPORT-02: User sees training completion summary with time, loss, save location
- TECH-04: cargo clippy -- -D warnings passes
- Module continues in infinite loop, restarting training after each completion
- All existing functionality (init, training, validation, checkpoints) still works
</success_criteria>

<output>
After completion, create `.planning/phases/04-export-polish/04-01-SUMMARY.md`
</output>
